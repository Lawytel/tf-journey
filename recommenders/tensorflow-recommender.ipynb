{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install tensorflow-recommenders\n!pip install scann","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-07T03:09:01.469604Z","iopub.execute_input":"2023-07-07T03:09:01.469995Z","iopub.status.idle":"2023-07-07T03:10:45.425131Z","shell.execute_reply.started":"2023-07-07T03:09:01.469965Z","shell.execute_reply":"2023-07-07T03:10:45.423210Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"### Import necessary libraries\n\nfrom typing import Dict, Text\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorflow_recommenders as tfrs\n\nimport time\nimport datetime\n\nimport os\nimport pprint\nimport tempfile\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:10:45.428059Z","iopub.execute_input":"2023-07-07T03:10:45.428502Z","iopub.status.idle":"2023-07-07T03:10:49.558911Z","shell.execute_reply.started":"2023-07-07T03:10:45.428464Z","shell.execute_reply":"2023-07-07T03:10:49.557769Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def reduce_size(df):\n    df = df.copy()\n    for col in df.select_dtypes(include='int64').columns:\n        df[col] = pd.to_numeric(df[col], downcast='unsigned')\n\n    for col in df.select_dtypes(include='object').columns:\n        df[col] = df[col].astype('category')\n\n    df.info(verbose=False)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:10:49.560415Z","iopub.execute_input":"2023-07-07T03:10:49.561198Z","iopub.status.idle":"2023-07-07T03:10:49.569396Z","shell.execute_reply.started":"2023-07-07T03:10:49.561159Z","shell.execute_reply":"2023-07-07T03:10:49.568303Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_1 = reduce_size(pd.read_csv(r'/kaggle/input/online-retail-ii-data-set-from-ml-repository/Year 2009-2010.csv', encoding='unicode_escape'))\ndf_2 = reduce_size(pd.read_csv(r'/kaggle/input/online-retail-ii-data-set-from-ml-repository/Year 2010-2011.csv', encoding='unicode_escape'))\nmaster_df = df_1.append(df_2, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:10:49.571581Z","iopub.execute_input":"2023-07-07T03:10:49.571911Z","iopub.status.idle":"2023-07-07T03:10:54.330025Z","shell.execute_reply.started":"2023-07-07T03:10:49.571885Z","shell.execute_reply":"2023-07-07T03:10:54.328892Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 525461 entries, 0 to 525460\nColumns: 8 entries, Invoice to Country\ndtypes: category(5), float64(2), int64(1)\nmemory usage: 19.3 MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 541910 entries, 0 to 541909\nColumns: 8 entries, Invoice to Country\ndtypes: category(5), float64(2), int64(1)\nmemory usage: 19.3 MB\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/3686948265.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  master_df = df_1.append(df_2, ignore_index = True)\n","output_type":"stream"}]},{"cell_type":"code","source":"master_df = master_df.dropna(subset=['Customer ID']).reset_index(drop=True)\nmaster_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:10:54.331148Z","iopub.execute_input":"2023-07-07T03:10:54.331456Z","iopub.status.idle":"2023-07-07T03:10:55.997628Z","shell.execute_reply.started":"2023-07-07T03:10:54.331431Z","shell.execute_reply":"2023-07-07T03:10:55.996526Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 824364 entries, 0 to 824363\nData columns (total 8 columns):\n #   Column       Non-Null Count   Dtype  \n---  ------       --------------   -----  \n 0   Invoice      824364 non-null  object \n 1   StockCode    824364 non-null  object \n 2   Description  824364 non-null  object \n 3   Quantity     824364 non-null  int64  \n 4   InvoiceDate  824364 non-null  object \n 5   Price        824364 non-null  float64\n 6   Customer ID  824364 non-null  float64\n 7   Country      824364 non-null  object \ndtypes: float64(2), int64(1), object(5)\nmemory usage: 50.3+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"user_df = master_df[['StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Customer ID', 'Country']]\nitem_df = master_df[['StockCode', 'Description']]","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:44:33.913117Z","iopub.execute_input":"2023-07-07T03:44:33.913570Z","iopub.status.idle":"2023-07-07T03:44:33.988507Z","shell.execute_reply.started":"2023-07-07T03:44:33.913538Z","shell.execute_reply":"2023-07-07T03:44:33.987402Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime\nuser_df['timestamp'] = user_df['InvoiceDate'].apply(lambda x: time.mktime(datetime.datetime.strptime(x, timestamp_format).timetuple()))\nuser_df['Customer ID'] = user_df['Customer ID'].astype(str)\nuser_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:44:34.966961Z","iopub.execute_input":"2023-07-07T03:44:34.967420Z","iopub.status.idle":"2023-07-07T03:44:55.548937Z","shell.execute_reply.started":"2023-07-07T03:44:34.967385Z","shell.execute_reply":"2023-07-07T03:44:55.547830Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/2099310759.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  user_df['timestamp'] = user_df['InvoiceDate'].apply(lambda x: time.mktime(datetime.datetime.strptime(x, timestamp_format).timetuple()))\n/tmp/ipykernel_32/2099310759.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  user_df['Customer ID'] = user_df['Customer ID'].astype(str)\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"  StockCode                          Description  Quantity     InvoiceDate  \\\n0     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12  12/1/2009 7:45   \n1    79323P                   PINK CHERRY LIGHTS        12  12/1/2009 7:45   \n2    79323W                  WHITE CHERRY LIGHTS        12  12/1/2009 7:45   \n3     22041         RECORD FRAME 7\" SINGLE SIZE         48  12/1/2009 7:45   \n4     21232       STRAWBERRY CERAMIC TRINKET BOX        24  12/1/2009 7:45   \n\n  Customer ID         Country     timestamp  \n0     13085.0  United Kingdom  1.259654e+09  \n1     13085.0  United Kingdom  1.259654e+09  \n2     13085.0  United Kingdom  1.259654e+09  \n3     13085.0  United Kingdom  1.259654e+09  \n4     13085.0  United Kingdom  1.259654e+09  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StockCode</th>\n      <th>Description</th>\n      <th>Quantity</th>\n      <th>InvoiceDate</th>\n      <th>Customer ID</th>\n      <th>Country</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>85048</td>\n      <td>15CM CHRISTMAS GLASS BALL 20 LIGHTS</td>\n      <td>12</td>\n      <td>12/1/2009 7:45</td>\n      <td>13085.0</td>\n      <td>United Kingdom</td>\n      <td>1.259654e+09</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>79323P</td>\n      <td>PINK CHERRY LIGHTS</td>\n      <td>12</td>\n      <td>12/1/2009 7:45</td>\n      <td>13085.0</td>\n      <td>United Kingdom</td>\n      <td>1.259654e+09</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>79323W</td>\n      <td>WHITE CHERRY LIGHTS</td>\n      <td>12</td>\n      <td>12/1/2009 7:45</td>\n      <td>13085.0</td>\n      <td>United Kingdom</td>\n      <td>1.259654e+09</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22041</td>\n      <td>RECORD FRAME 7\" SINGLE SIZE</td>\n      <td>48</td>\n      <td>12/1/2009 7:45</td>\n      <td>13085.0</td>\n      <td>United Kingdom</td>\n      <td>1.259654e+09</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21232</td>\n      <td>STRAWBERRY CERAMIC TRINKET BOX</td>\n      <td>24</td>\n      <td>12/1/2009 7:45</td>\n      <td>13085.0</td>\n      <td>United Kingdom</td>\n      <td>1.259654e+09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"interactions_dict = user_df.groupby(['Customer ID', 'StockCode', 'Description', 'timestamp', 'Country'])[ 'Quantity'].sum().reset_index()\n\ninteractions_dict = {name: np.array(value) for name, value in interactions_dict.items()}\ninteractions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n\nitems_dict = item_df[['StockCode', 'Description']].drop_duplicates(subset=['StockCode'])\n\nitems_dict = {name: np.array(value) for name, value in items_dict.items()}\nitems = tf.data.Dataset.from_tensor_slices(items_dict)\n\ninteractions = interactions.map(lambda x: {\n                                            'user_id' : x['Customer ID'], \n                                            'item_id' : x['StockCode'], \n                                            'quantity' : float(x['Quantity']),\n                                            \"timestamp\": x[\"timestamp\"],\n                                            'country': x['Country'],\n                                            'item_name': x['Description'],\n                                        })\n\nitems = items.map(lambda x: {'item_id': x['StockCode'],\n                             'item_name': x['Description'],\n                            })","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:44:55.551135Z","iopub.execute_input":"2023-07-07T03:44:55.551508Z","iopub.status.idle":"2023-07-07T03:44:56.879969Z","shell.execute_reply.started":"2023-07-07T03:44:55.551478Z","shell.execute_reply":"2023-07-07T03:44:56.878747Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"## Basic housekeeping to prepare feature vocabularies\n\n## timestamp is an exmaple of continuous features, which needs to be rescaled, or otherwise it will be \n## too large for the model.\n## there are other methods to reduce the size of the timestamp, ,such as standardization and normalization\n## here we use discretization, which puts them into buckets of categorical features, \n\ntimestamps = np.concatenate(list(interactions.map(lambda x: x[\"timestamp\"]).batch(100)))\nmax_timestamp = timestamps.max()\nmin_timestamp = timestamps.min()\ntimestamp_buckets = np.linspace(\n    min_timestamp, max_timestamp, num=1000,)\n\nunique_user_ids = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"user_id\"]))))\nunique_user_quantity = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"quantity\"]))))\nunique_user_country = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"country\"]))))\n\nunique_item_ids = np.unique(np.concatenate(list(items.batch(1_000).map(lambda x: x[\"item_id\"]))))\nunique_item_name = np.unique(np.concatenate(list(items.batch(1_000).map(lambda x: x[\"item_name\"]))))\n\n# item_titles = interactions.batch(10_000).map(lambda x: x[\"item_id\"])\n# user_ids = interactions.batch(10_000).map(lambda x: x[\"user_id\"])\n\n# unique_item_titles = np.unique(np.concatenate(list(item_titles)))\n# unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n\n# unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n \n# unique_user_gender = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n#     lambda x: x[\"user_gender\"]))))\n \n# unique_bucketized_user_age = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n#     lambda x: x[\"bucketized_user_age\"]))))\n# unique_user_occupation_label = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n#     lambda x: x[\"user_occupation_label\"]))))","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:44:56.881410Z","iopub.execute_input":"2023-07-07T03:44:56.881753Z","iopub.status.idle":"2023-07-07T03:46:21.785020Z","shell.execute_reply.started":"2023-07-07T03:44:56.881724Z","shell.execute_reply":"2023-07-07T03:46:21.783892Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"class UserModel(tf.keras.Model):\n   \n  def __init__(self):\n    super().__init__()\n    \n    self.user_embedding = tf.keras.Sequential([\n        tf.keras.layers.StringLookup(\n            vocabulary=unique_user_ids, mask_token=None),\n        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n    ])\n \n    self.country_embedding = tf.keras.Sequential([\n        tf.keras.layers.StringLookup(\n            vocabulary=unique_user_country, mask_token=None),\n        tf.keras.layers.Embedding(len(unique_user_country) + 1, 32),\n    ])\n         \n    self.timestamp_embedding = tf.keras.Sequential([\n        tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n        tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n    ])\n    self.normalized_timestamp = tf.keras.layers.Normalization(\n        axis=None\n    )\n \n    self.normalized_timestamp.adapt(timestamps)\n \n  def call(self, inputs):\n    # Take the input dictionary, pass it through each input layer,\n    # and concatenate the result.\n    return tf.concat([\n        self.user_embedding(inputs[\"user_id\"]),\n        self.country_embedding(inputs[\"country\"]),\n        self.timestamp_embedding(inputs[\"timestamp\"]),\n        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n    ], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:46:21.787716Z","iopub.execute_input":"2023-07-07T03:46:21.788047Z","iopub.status.idle":"2023-07-07T03:46:21.798073Z","shell.execute_reply.started":"2023-07-07T03:46:21.788020Z","shell.execute_reply":"2023-07-07T03:46:21.797251Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"class QueryModel(tf.keras.Model):\n  '\"\"\"Model for encoding user queries.\"\"\"'\n \n  def __init__(self, layer_sizes):\n    \"\"\"Model for encoding user queries.\n \n    Args:\n      layer_sizes:\n        A list of integers where the i-th entry represents the number of units\n        the i-th layer contains.\n    \"\"\"\n    super().__init__()\n \n    # We first use the user model for generating embeddings.\n    self.embedding_model = UserModel()\n \n    # Then construct the layers.\n    self.dense_layers = tf.keras.Sequential()\n \n    # Use the ReLU activation for all but the last layer.\n    for layer_size in layer_sizes[:-1]:\n      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n \n    # No activation for the last layer.\n    for layer_size in layer_sizes[-1:]:\n      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n     \n  def call(self, inputs):\n    feature_embedding = self.embedding_model(inputs)\n    return self.dense_layers(feature_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T03:46:21.799273Z","iopub.execute_input":"2023-07-07T03:46:21.799808Z","iopub.status.idle":"2023-07-07T03:46:21.824006Z","shell.execute_reply.started":"2023-07-07T03:46:21.799779Z","shell.execute_reply":"2023-07-07T03:46:21.822912Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class ItemModel(tf.keras.Model):\n   \n  def __init__(self):\n    super().__init__()\n \n    max_tokens = 10_000\n    \n    self.item_embedding = tf.keras.Sequential([\n        tf.keras.layers.StringLookup(\n            vocabulary=unique_item_ids, mask_token=None),\n        tf.keras.layers.Embedding(len(unique_item_ids) + 1, 32),\n    ])\n \n    self.name_embedding = tf.keras.Sequential([\n      tf.keras.layers.StringLookup(\n          vocabulary=unique_item_name,mask_token=None),\n      tf.keras.layers.Embedding(len(unique_item_name) + 1, 32)\n    ])\n \n    self.name_vectorizer = tf.keras.layers.TextVectorization(\n        max_tokens=max_tokens)\n \n    self.name_text_embedding = tf.keras.Sequential([\n      self.name_vectorizer,\n      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n      tf.keras.layers.GlobalAveragePooling1D(),\n    ])\n \n    self.name_vectorizer.adapt(items.map(lambda x: x['item_name']))\n \n  def call(self, inputs):\n    return tf.concat([\n        self.item_embedding(inputs['item_id']),\n        self.name_embedding(inputs['item_name']),\n        self.name_text_embedding(inputs['item_name']),\n    ], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:05:57.542863Z","iopub.execute_input":"2023-07-07T04:05:57.543370Z","iopub.status.idle":"2023-07-07T04:05:57.554920Z","shell.execute_reply.started":"2023-07-07T04:05:57.543321Z","shell.execute_reply":"2023-07-07T04:05:57.553719Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"class CandidateModel(tf.keras.Model):\n  \"\"\"Model for encoding items.\"\"\"\n \n  def __init__(self, layer_sizes):\n    \"\"\"Model for encoding items.\n \n    Args:\n      layer_sizes:\n        A list of integers where the i-th entry represents the number of units\n        the i-th layer contains.\n    \"\"\"\n    super().__init__()\n \n    self.embedding_model = ItemModel()\n \n    # Then construct the layers.\n    self.dense_layers = tf.keras.Sequential()\n \n    # Use the ReLU activation for all but the last layer.\n    for layer_size in layer_sizes[:-1]:\n      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n \n    # No activation for the last layer.\n    for layer_size in layer_sizes[-1:]:\n      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n     \n  def call(self, inputs):\n    feature_embedding = self.embedding_model(inputs)\n    return self.dense_layers(feature_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:05:57.753528Z","iopub.execute_input":"2023-07-07T04:05:57.753987Z","iopub.status.idle":"2023-07-07T04:05:57.763283Z","shell.execute_reply.started":"2023-07-07T04:05:57.753933Z","shell.execute_reply":"2023-07-07T04:05:57.762421Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"class ItemlensModel(tfrs.models.Model):\n \n  def __init__(self, layer_sizes):\n    super().__init__()\n    self.query_model = QueryModel(layer_sizes)\n    self.candidate_model = CandidateModel(layer_sizes)\n    self.task = tfrs.tasks.Retrieval(\n        metrics=tfrs.metrics.FactorizedTopK(\n            candidates=items.batch(128).map(self.candidate_model),\n        ),\n    )\n \n  def compute_loss(self, features, training=False):\n \n    query_embeddings = self.query_model({\n        \"user_id\": features[\"user_id\"],\n        \"country\": features[\"country\"],\n        \"timestamp\": features[\"timestamp\"],\n    })\n    item_embeddings = self.candidate_model({\n        \"item_id\": features[\"item_id\"],\n        \"item_name\": features[\"item_name\"],\n    })\n \n    return self.task(\n        query_embeddings, item_embeddings, compute_metrics=not training)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:05:57.949815Z","iopub.execute_input":"2023-07-07T04:05:57.950242Z","iopub.status.idle":"2023-07-07T04:05:57.959523Z","shell.execute_reply.started":"2023-07-07T04:05:57.950208Z","shell.execute_reply":"2023-07-07T04:05:57.958312Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nshuffled = interactions.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n\ntrain = shuffled.take(80_000)\ntest = shuffled.skip(80_000).take(20_000)\n \ncached_train = train.shuffle(100_000).batch(2048)\ncached_test = test.batch(4096).cache()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:05:58.127177Z","iopub.execute_input":"2023-07-07T04:05:58.127840Z","iopub.status.idle":"2023-07-07T04:05:58.168550Z","shell.execute_reply.started":"2023-07-07T04:05:58.127804Z","shell.execute_reply":"2023-07-07T04:05:58.167658Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50\n \nmodel = ItemlensModel([32])\nmodel.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n \none_layer_history = model.fit(\n    cached_train,\n    validation_data=cached_test,\n    validation_freq=5,\n    epochs=num_epochs,\n    verbose=0)\n \naccuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\nprint(f\"Top-100 accuracy: {accuracy:.2f}.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:05:58.281254Z","iopub.execute_input":"2023-07-07T04:05:58.281965Z","iopub.status.idle":"2023-07-07T04:16:03.789366Z","shell.execute_reply.started":"2023-07-07T04:05:58.281931Z","shell.execute_reply":"2023-07-07T04:16:03.788024Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"Top-100 accuracy: 0.29.\n","output_type":"stream"}]},{"cell_type":"code","source":"index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\nindex.index_from_dataset(\n  tf.data.Dataset.zip((items.batch(100).map(lambda x: x[\"item_name\"]), items.batch(100).map(model.candidate_model)))\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:28:46.790984Z","iopub.execute_input":"2023-07-07T04:28:46.791532Z","iopub.status.idle":"2023-07-07T04:28:47.047765Z","shell.execute_reply.started":"2023-07-07T04:28:46.791497Z","shell.execute_reply":"2023-07-07T04:28:47.046454Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"_, titles = index({\n    \"user_id\": np.array(['13085.0']),\n    \"country\": np.array([\"United Kingdom\"]),\n    \"timestamp\": np.array([1.259654e+09])},\n    k=50\n)\ntitles[0].numpy()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:33:31.830782Z","iopub.execute_input":"2023-07-07T04:33:31.831282Z","iopub.status.idle":"2023-07-07T04:33:31.856479Z","shell.execute_reply.started":"2023-07-07T04:33:31.831250Z","shell.execute_reply":"2023-07-07T04:33:31.855180Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"array([b'LUNCHBOX WITH CUTLERY FAIRY CAKES ',\n       b'FANCY FONT HOME SWEET HOME DOORMAT',\n       b'UNION JACK GUNS & ROSES  DOORMAT',\n       b'SALT AND PEPPER SHAKERS TOADSTOOLS',\n       b'72 CAKE CASES VINTAGE CHRISTMAS',\n       b'WOOLLY HAT SOCK GLOVE ADVENT STRING', b'STRAWBERRY CANDY BAG',\n       b'PACK OF 20 FAIRY CAKE PAPER NAPKINS', b'RED SPOTTY COIR DOORMAT',\n       b'RETRO \"TEA FOR ONE\" ', b'TRIANGULAR POUFFE VINTAGE ',\n       b'6 CROCHET STRAWBERRIES', b'UNION JACK HOT WATER BOTTLE ',\n       b'DINOSAURS WATER TRANSFER TATTOOS ',\n       b'VINTAGE SEASIDE JIGSAW PUZZLES', b'SET 10 LIGHTS NIGHT OWL',\n       b'WHITE HANGING BEADS CANDLE HOLDER', b'MILK PAN BLUE RETROSPOT',\n       b'FRYING PAN BLUE POLKADOT ',\n       b'JUNGLE POPSICLES ICE LOLLY HOLDERS', b' WHITE CHERRY LIGHTS',\n       b'VINTAGE SNAP CARDS', b'CURIOUS  IMAGES NOTEBOOK SET',\n       b'WRAP BLIZZARD', b'HANGING FAIRY CAKE DECORATION',\n       b'SET OF MEADOW  FLOWER STICKERS', b'RED SPOTTY ROUND CAKE TINS',\n       b'RED SPOT HEART HOT WATER BOTTLE',\n       b'PIG KEYRING WITH LIGHT & SOUND ',\n       b'BABUSHKA LIGHTS STRING OF 10',\n       b'GLASS ETCHED T-LIGHT HOLDER LARGE',\n       b'POLYESTER FILLER PAD 30CMx30CM',\n       b'VINTAGE HEADS AND TAILS CARD GAME ',\n       b'CHICK GREY HOT WATER BOTTLE', b' FLAMINGO LIGHTS',\n       b'JAPANESE CROCHETED ANIMAL', b' RED/WHITE DOT MINI CASES',\n       b'ZINC TOP  2 DOOR WOODEN SHELF ',\n       b'CAKE STAND LOVEBIRD 2 TIER WHITE',\n       b'PINK  HEART SHAPE LOVE BUCKET ',\n       b'WOODLAND  HEIGHT CHART STICKERS ', b'WRAP BLUE REINDEER',\n       b'BAG 250g SWIRLY MARBLES', b'COSY SLIPPER SHOES LARGE RED',\n       b'BIRD DECORATION RED SPOT', b'CARD, GINGHAM ROSE ',\n       b'FLOWER FAIRY SUMMER BOUQUET SACHET', b'BAG 125g SWIRLY MARBLES',\n       b'SET OF THREE VINTAGE GIFT WRAPS', b'BLUE DINER WALL CLOCK'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"_, titles = index({\n    \"user_id\": np.array(['12680.0']),\n    \"country\": np.array([\"France\"]),\n    \"timestamp\": np.array([1.323435e+09])},\n    k=50\n)\n\ntitles[0].numpy()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:34:53.469123Z","iopub.execute_input":"2023-07-07T04:34:53.469768Z","iopub.status.idle":"2023-07-07T04:34:53.496348Z","shell.execute_reply.started":"2023-07-07T04:34:53.469728Z","shell.execute_reply":"2023-07-07T04:34:53.495013Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"array([b'CHILDRENS CUTLERY CIRCUS PARADE', b'CHILDRENS CUTLERY SPACEBOY ',\n       b'ALARM CLOCK BAKELIKE GREEN', b'ALARM CLOCK BAKELIKE IVORY',\n       b'ALARM CLOCK BAKELIKE PINK', b'BOYS VINTAGE TIN SEASIDE BUCKET',\n       b'RED METAL BEACH SPADE ', b'CHILDRENS CUTLERY DOLLY GIRL ',\n       b'MAGIC DRAWING SLATE SPACEBOY ', b'ALARM CLOCK BAKELIKE RED ',\n       b'PLASTERS IN TIN CIRCUS PARADE ',\n       b'WOODLAND BUNNIES LOLLY MAKERS', b'POSTAGE',\n       b'DINOSAUR HEIGHT CHART STICKER SET',\n       b'BAKING SET 9 PIECE RETROSPOT ', b'BLUE HARMONICA IN BOX ',\n       b'PASTEL COLOUR HONEYCOMB FAN', b'COSY SLIPPER SHOES LARGE GREEN',\n       b'FAIRY CAKE BIRTHDAY CANDLE SET', b'SET OF 6 SOLDIER SKITTLES',\n       b'PINK DINER WALL CLOCK', b'ALARM CLOCK BAKELIKE CHOCOLATE',\n       b'GINGERBREAD MAN COOKIE CUTTER', b'4 TRADITIONAL SPINNING TOPS',\n       b'DOLLY GIRL BABY GIFT SET', b'TREASURE ISLAND BOOK BOX',\n       b'HOLIDAY FUN LUDO', b'MINI JIGSAW DOLLY GIRL',\n       b'GIRLS VINTAGE TIN SEASIDE BUCKET',\n       b'HEADS AND TAILS SPORTING FUN', b'PLASTERS IN TIN STRONGMAN',\n       b'3 TRADITIONAl BISCUIT CUTTERS  SET', b'PINK ROSE WASHBAG',\n       b'PACK OF 12 WOODLAND TISSUES ', b'CARD DOLLY GIRL ',\n       b'MINI JIGSAW SPACEBOY', b'SET/4 BADGES BALLOON GIRL',\n       b'CHILDS BREAKFAST SET DOLLY GIRL ', b'CARD BIRTHDAY COWBOY',\n       b'GUMBALL COAT RACK', b'BOX OF VINTAGE ALPHABET BLOCKS',\n       b'MINI JIGSAW CIRCUS PARADE ',\n       b'TRADITIONAL WOODEN CATCH CUP GAME ',\n       b'CHILDRENS APRON DOLLY GIRL ', b'BOX OF VINTAGE JIGSAW BLOCKS ',\n       b'EASTER TIN BUNNY BOUQUET', b'PLASTERS IN TIN WOODLAND ANIMALS',\n       b'BAKING SET SPACEBOY DESIGN', b'MINI JIGSAW BAKE A CAKE ',\n       b'PLASTERS IN TIN SPACEBOY'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"user_df[user_df['Customer ID'] == '12680.0'].head(20)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:35:07.350036Z","iopub.execute_input":"2023-07-07T04:35:07.350553Z","iopub.status.idle":"2023-07-07T04:35:07.533457Z","shell.execute_reply.started":"2023-07-07T04:35:07.350514Z","shell.execute_reply":"2023-07-07T04:35:07.532190Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"       StockCode                          Description  Quantity  \\\n638501     21981         PACK OF 12 WOODLAND TISSUES         24   \n638502     21986     PACK OF 12 PINK POLKADOT TISSUES        24   \n638503     22037                  ROBOT BIRTHDAY CARD        12   \n638504     23190  BUNDLE OF 3 SCHOOL EXERCISE BOOKS          12   \n638505     22555            PLASTERS IN TIN STRONGMAN        12   \n638506     22629                  SPACEBOY LUNCH BOX         12   \n638507     22980               PANTRY SCRUBBING BRUSH        12   \n638508     22979              PANTRY WASHING UP BRUSH        12   \n638509     22712                     CARD DOLLY GIRL         12   \n638510     22630                 DOLLY GIRL LUNCH BOX        12   \n638511     22899         CHILDREN'S APRON DOLLY GIRL          6   \n638512     23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n638513     22367      CHILDRENS APRON SPACEBOY DESIGN         8   \n638514     22029               SPACEBOY BIRTHDAY CARD        12   \n638515     23256          CHILDRENS CUTLERY SPACEBOY          4   \n638516     22728            ALARM CLOCK BAKELIKE PINK         4   \n638517     22727            ALARM CLOCK BAKELIKE RED          4   \n638518     22099            CARAVAN SQUARE TISSUE BOX        36   \n638519     22326  ROUND SNACK BOXES SET OF4 WOODLAND          6   \n638520      POST                              POSTAGE         2   \n\n            InvoiceDate Customer ID Country     timestamp  \n638501  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638502  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638503  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638504  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638505  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638506  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638507  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638508  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638509  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638510  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638511  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638512  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638513  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638514  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638515  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638516  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638517  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638518  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638519  8/18/2011 15:44     12680.0  France  1.313682e+09  \n638520  8/18/2011 15:44     12680.0  France  1.313682e+09  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StockCode</th>\n      <th>Description</th>\n      <th>Quantity</th>\n      <th>InvoiceDate</th>\n      <th>Customer ID</th>\n      <th>Country</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>638501</th>\n      <td>21981</td>\n      <td>PACK OF 12 WOODLAND TISSUES</td>\n      <td>24</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638502</th>\n      <td>21986</td>\n      <td>PACK OF 12 PINK POLKADOT TISSUES</td>\n      <td>24</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638503</th>\n      <td>22037</td>\n      <td>ROBOT BIRTHDAY CARD</td>\n      <td>12</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638504</th>\n      <td>23190</td>\n      <td>BUNDLE OF 3 SCHOOL EXERCISE BOOKS</td>\n      <td>12</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638505</th>\n      <td>22555</td>\n      <td>PLASTERS IN TIN STRONGMAN</td>\n      <td>12</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638506</th>\n      <td>22629</td>\n      <td>SPACEBOY LUNCH BOX</td>\n      <td>12</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638507</th>\n      <td>22980</td>\n      <td>PANTRY SCRUBBING BRUSH</td>\n      <td>12</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638508</th>\n      <td>22979</td>\n      <td>PANTRY WASHING UP BRUSH</td>\n      <td>12</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638509</th>\n      <td>22712</td>\n      <td>CARD DOLLY GIRL</td>\n      <td>12</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638510</th>\n      <td>22630</td>\n      <td>DOLLY GIRL LUNCH BOX</td>\n      <td>12</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638511</th>\n      <td>22899</td>\n      <td>CHILDREN'S APRON DOLLY GIRL</td>\n      <td>6</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638512</th>\n      <td>23254</td>\n      <td>CHILDRENS CUTLERY DOLLY GIRL</td>\n      <td>4</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638513</th>\n      <td>22367</td>\n      <td>CHILDRENS APRON SPACEBOY DESIGN</td>\n      <td>8</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638514</th>\n      <td>22029</td>\n      <td>SPACEBOY BIRTHDAY CARD</td>\n      <td>12</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638515</th>\n      <td>23256</td>\n      <td>CHILDRENS CUTLERY SPACEBOY</td>\n      <td>4</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638516</th>\n      <td>22728</td>\n      <td>ALARM CLOCK BAKELIKE PINK</td>\n      <td>4</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638517</th>\n      <td>22727</td>\n      <td>ALARM CLOCK BAKELIKE RED</td>\n      <td>4</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638518</th>\n      <td>22099</td>\n      <td>CARAVAN SQUARE TISSUE BOX</td>\n      <td>36</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638519</th>\n      <td>22326</td>\n      <td>ROUND SNACK BOXES SET OF4 WOODLAND</td>\n      <td>6</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n    <tr>\n      <th>638520</th>\n      <td>POST</td>\n      <td>POSTAGE</td>\n      <td>2</td>\n      <td>8/18/2011 15:44</td>\n      <td>12680.0</td>\n      <td>France</td>\n      <td>1.313682e+09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import tempfile\nimport os\n# Export the query model.\nwith tempfile.TemporaryDirectory() as tmp:\n  path = os.path.join(tmp, \"model\")\n  \n  # Save the index.\n  tf.saved_model.save(index, path)\n  \n  # Load it back; can also be done in TensorFlow Serving.\n  loaded = tf.saved_model.load(path)\n  \n  # Pass a user id in, get top predicted movie titles back.\n  _, titles = loaded({\n    \"bucketized_user_age\": np.array([25]),\n    \"user_occupation_label\": np.array([17]),\n    \"user_gender\": np.array([True]),\n    \"timestamp\": np.array([879024327])}\n)\n  \n  print(f\"Recommendations: {titles[0][:10]}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\npkl_model = \"model.h5\"\npkl_index = 'index.h5'\n\n# with open(pkl_model, 'wb') as file:  \n#     pickle.dump(model, file)\n\nwith open(pkl_index, 'wb') as file:     \n    pickle.dump(index, file)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T04:47:51.544702Z","iopub.execute_input":"2023-07-07T04:47:51.545123Z","iopub.status.idle":"2023-07-07T04:47:51.943954Z","shell.execute_reply.started":"2023-07-07T04:47:51.545076Z","shell.execute_reply":"2023-07-07T04:47:51.942748Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......query_model\n.........dense_layers\n............layers\n...............dense\n..................vars\n.....................0\n.....................1\n............vars\n.........embedding_model\n............country_embedding\n...............layers\n..................embedding\n.....................vars\n........................0\n..................string_lookup\n.....................vars\n...............vars\n............layers\n...............normalization\n..................vars\n.....................0\n.....................1\n.....................2\n...............sequential\n..................layers\n.....................embedding\n........................vars\n...........................0\n.....................string_lookup\n........................vars\n..................vars\n...............sequential_2\n..................layers\n.....................discretization\n........................vars\n.....................embedding\n........................vars\n...........................0\n..................vars\n............vars\n.........vars\n...vars\n......0\n......1\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-07-07 04:47:51          207\nvariables.h5                                   2023-07-07 04:47:51      1844088\nmetadata.json                                  2023-07-07 04:47:51           64\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}